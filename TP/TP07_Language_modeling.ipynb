{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/MAP583/blob/main/TP/TP07_Language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc78654-2c72-41c1-acbd-8b91ef66b3e8",
      "metadata": {
        "id": "cfc78654-2c72-41c1-acbd-8b91ef66b3e8"
      },
      "source": [
        "# TP07 - Language Modeling\n",
        "\n",
        "- Part A is a short introduction to language modeling with Markov chains.\n",
        "- Part B is optional, conceptually easy, but more challenging in terms of programming.\n",
        "- Part C is the most interesting, and the goal of this session, adjust your time budget accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7289c688-74a8-45fa-aa60-38d6883d3eec",
      "metadata": {
        "id": "7289c688-74a8-45fa-aa60-38d6883d3eec"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet \"git+https://gitlab.com/robindar/dl-scaman_checker.git\"\n",
        "from dl_scaman_checker import TP07\n",
        "TP07.check_install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466c42b2-f5f9-4a7c-9f30-3d2e4d37c6bf",
      "metadata": {
        "id": "466c42b2-f5f9-4a7c-9f30-3d2e4d37c6bf"
      },
      "outputs": [],
      "source": [
        "filename = \"input.txt\"\n",
        "TP07.download_tinyshakespeare(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdd53d5-7bc9-4b21-bc66-dd7835e9d8a7",
      "metadata": {
        "id": "ffdd53d5-7bc9-4b21-bc66-dd7835e9d8a7"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F, numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7942e7-abca-4ec0-9f3c-ef88bc2a8742",
      "metadata": {
        "id": "bf7942e7-abca-4ec0-9f3c-ef88bc2a8742"
      },
      "source": [
        "## Part A - Language modeling basics\n",
        "\n",
        "### A.1 - Predictive models for textual data\n",
        "\n",
        "Our goal is to generate random text. To this end, we will define a probability distribution\n",
        "over \"texts\", such that we are able to sample from this distribution, and then optimize over\n",
        "such distributions.\n",
        "\n",
        "Let $\\Sigma$ be a set, called the \"alphabet\".\n",
        "A \"text\" of size $k \\in \\mathbb{N}$ is an element of $\\Sigma^k$.\n",
        "Define $X : \\mathbb{N} \\to \\Sigma$ a sequence of random variables with a given distribution.\n",
        "We model the probability of a text $t \\in \\Sigma^k$\n",
        "as $$\\mathbb{P}(t) = \\mathbb{P}(X_{\\leq k} = t) = \\prod_{i \\in [k]} \\mathbb{P}(X_i = t_i \\mid X_{< i} = t_{< i}) $$\n",
        "For every distribution on sequences, this yields a simple way to sample: one character at a time.\n",
        "\n",
        "We will represent these conditional distributions as a function\n",
        "$f_i : \\Sigma^i \\to \\mathbb{R}^\\Sigma$ for $i \\in \\mathbb{N}$,\n",
        "such that $$\\mathbb{P}(X_i = s \\,|\\, X_{<i} = q) = \\frac{ \\exp\\left(r_s\\right) }{ \\sum_{j \\in \\Sigma} \\exp\\left(r_j\\right) } \\quad\\text{where}\\quad r = f_i(q)$$\n",
        "This transformation is implemented as `F.softmax` in PyTorch. The variable `r` is called the \"logit\".\n",
        "Usually the function $f$ only depends on a finite number of previous characters, called the \"block size\" of the model.\n",
        "For instance, a Markov chain has a block size of 1. This influences the computational cost of the model, but also the size of the context that it can take into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85da5a1d-b869-47ca-b982-2eb1a8df702c",
      "metadata": {
        "id": "85da5a1d-b869-47ca-b982-2eb1a8df702c"
      },
      "outputs": [],
      "source": [
        "alphabet, train_data, val_data = TP07.read_from_file(filename)\n",
        "vocab_size = len(alphabet.characters)\n",
        "alphabet.characters, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e59223-0220-4f60-b018-dabfc7de7abd",
      "metadata": {
        "id": "74e59223-0220-4f60-b018-dabfc7de7abd"
      },
      "outputs": [],
      "source": [
        "train_data.shape, train_data.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945b0a3e-6159-41ce-8f8b-32406ad13eb5",
      "metadata": {
        "id": "945b0a3e-6159-41ce-8f8b-32406ad13eb5"
      },
      "outputs": [],
      "source": [
        "train_size, val_size = [ TP07.human_readable_unit(len(d)) for d in (train_data, val_data) ]\n",
        "print(f\"Training: {train_size} chars, Validation: {val_size} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a704fa62-fb17-452f-8091-3d8c6c5920fb",
      "metadata": {
        "id": "a704fa62-fb17-452f-8091-3d8c6c5920fb"
      },
      "source": [
        "We provide the `Alphabet` class, which performs the encoding / decoding between strings and sequences of integers, with the functions `alphabet.encode` and `alphabet.decode`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f150921-2c4a-40b6-95bb-5746c43f7347",
      "metadata": {
        "id": "7f150921-2c4a-40b6-95bb-5746c43f7347"
      },
      "outputs": [],
      "source": [
        "alphabet.encode(\"Lorem ipsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad716a3-e9e0-4cc7-8d5b-f7a3cbc12edb",
      "metadata": {
        "id": "fad716a3-e9e0-4cc7-8d5b-f7a3cbc12edb"
      },
      "outputs": [],
      "source": [
        "alphabet.decode([ 24, 53, 56, 43, 51 ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a6b8fd-c69c-49c2-8af3-dbd84cda87a8",
      "metadata": {
        "id": "94a6b8fd-c69c-49c2-8af3-dbd84cda87a8"
      },
      "source": [
        "### A.2 - Sampling from a given language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be5ec3d-9827-4ee6-9020-a8dcfa32b2b9",
      "metadata": {
        "id": "5be5ec3d-9827-4ee6-9020-a8dcfa32b2b9"
      },
      "outputs": [],
      "source": [
        "sample_model = TP07.SampleLogitModel(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba446a15-a7cb-4256-8df9-a9cbb8882328",
      "metadata": {
        "id": "ba446a15-a7cb-4256-8df9-a9cbb8882328"
      },
      "outputs": [],
      "source": [
        "sample_model(alphabet.encode(\"brow\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda9b7b6-3b10-43c7-813e-412f03a6ab23",
      "metadata": {
        "id": "dda9b7b6-3b10-43c7-813e-412f03a6ab23"
      },
      "source": [
        "Write the `prompt` function, which takes a logit-computation function $f$, a number of characters to sample, and a preprompt to initialize the sequence. Additionally, take a temperature parameter $T \\in \\mathbb{R}_+^*$, and sample from the logits $s \\mapsto \\frac{1}{T} f(s)$, such that in the limit $T \\to +\\infty$ it samples from the uniform distribution.\n",
        "\n",
        "You can use the functions `F.softmax` and `torch.multinomial(p, num_samples=1)` for sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417ed8ab-50df-4818-86cb-fac12da1b14c",
      "metadata": {
        "id": "417ed8ab-50df-4818-86cb-fac12da1b14c"
      },
      "outputs": [],
      "source": [
        "def prompt(model, num_chars, preprompt=\"\", temp=1.):\n",
        "    ### your code here ###\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c3838d-9e0b-4623-a490-6d04cef0705b",
      "metadata": {
        "id": "b6c3838d-9e0b-4623-a490-6d04cef0705b"
      },
      "source": [
        "Generate a couple sentences (e.g. 80 characters) from the sample model, with the preprompts \"quick\" and with a non-sensical preprompt such as \"sjdaklda\". What do you notice when you increase the temperature ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9733f116-5f8d-485b-99c3-e570623cb845",
      "metadata": {
        "id": "9733f116-5f8d-485b-99c3-e570623cb845"
      },
      "outputs": [],
      "source": [
        "### you code here ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fb30bd-f5f6-4d33-8ebc-630b0901beeb",
      "metadata": {
        "id": "e9fb30bd-f5f6-4d33-8ebc-630b0901beeb"
      },
      "source": [
        "### A.3 - Building a bigram model\n",
        "\n",
        "For your first custom model, represent text with a Markov chain over the alphabet, where the probability of each character in the text depends only on the previous character. Use as transition matrix the empirical distribution of pairs of characters in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e00d15fe-b85f-4d91-9c28-d95edf79f92b",
      "metadata": {
        "id": "e00d15fe-b85f-4d91-9c28-d95edf79f92b"
      },
      "outputs": [],
      "source": [
        "### your code here ###\n",
        "logits = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a21590d-cc48-4e58-9374-31815603acee",
      "metadata": {
        "id": "1a21590d-cc48-4e58-9374-31815603acee"
      },
      "outputs": [],
      "source": [
        "class StaticBigramModel():\n",
        "    def __init__(self, logits):\n",
        "        self.logits = logits\n",
        "\n",
        "    def __call__(self, seq):\n",
        "        if len(seq) == 0:\n",
        "            return self.logits[0,:]\n",
        "        return self.logits[seq[-1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d466e0-9a06-402c-bee3-a7263fbc37b7",
      "metadata": {
        "id": "05d466e0-9a06-402c-bee3-a7263fbc37b7"
      },
      "outputs": [],
      "source": [
        "static_bigram_model = StaticBigramModel(logits)\n",
        "print(prompt(static_bigram_model, 80))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b821205a-90d1-4a13-ac11-aadce385ad8b",
      "metadata": {
        "id": "b821205a-90d1-4a13-ac11-aadce385ad8b"
      },
      "source": [
        "The result is quite disappointing, but the bigram model is also clearly an oversimplification of language, it would be very surprising if it could just spit out entire paragraphs from Shakespeare.\n",
        "To assess performance in an easier setting, prompt this model with `\"Jul\"`, and see how often you can get it to auto-complete `\"Juliet\"` (a name which appears a lot in Shakespeare's plays). What happens if you lower the temperature too much ? How is this different from what happens when you prompt with `\"the \"` ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d81ddc7-e040-48b6-aea1-1cbeac706f7d",
      "metadata": {
        "id": "1d81ddc7-e040-48b6-aea1-1cbeac706f7d"
      },
      "outputs": [],
      "source": [
        "### your code here ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e1c4f3-1e51-4be6-a3aa-b27e8030b1c9",
      "metadata": {
        "id": "12e1c4f3-1e51-4be6-a3aa-b27e8030b1c9"
      },
      "source": [
        "## Part B - N-Gram Language modeling (optional, advanced)\n",
        "\n",
        "An $n$-gram model for $n \\in \\mathbb{N} \\setminus \\{0\\}$ models text not with a Markov chain over the alphabet, but with a Markov chain over $n$-grams (tuples of $n$ consecutive characters).\n",
        "It defines $Y : \\mathbb{N} \\to \\Sigma^n$ the sequence $Y_i = (X_{i+j})_{j \\in [n]}$\n",
        "and constructs the distribution on $X$ from a transition probability matrix for $Y$.\n",
        "This is a standard trick of Markov chains to depend on the previous $(n-1)$ events instead of only the last event.\n",
        "\n",
        "Write your own $n$-gram model, which takes at initialization an integer $n \\in \\mathbb{N} \\setminus \\{0\\}$ and the training data, then compute the empirical transition probabilities for $Y$. You should have a memory complexity of $(\\#\\Sigma)^n$.\n",
        "\n",
        "A few tricks to help you deal with varying $n$:\n",
        "- For `a` of type `np.ndarray`, the syntaxes `a[0,4,2]` and `a[tuple(0,4,2)]` are equivalent\n",
        "- You can cast back and forth from list to tuples with `list(t)` and `tuple(l)`\n",
        "\n",
        "To avoid dealing with preprompts that are too short, you can pad with leading space characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772e582a-3c3d-4160-9bc3-19be1040c19e",
      "metadata": {
        "id": "772e582a-3c3d-4160-9bc3-19be1040c19e"
      },
      "outputs": [],
      "source": [
        "class StaticNGramModel():\n",
        "    def __init__(self, n, train_data):\n",
        "        pass\n",
        "\n",
        "    ### your code here ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044957ff-8ed6-4fc7-9827-55ebf1f8b3a2",
      "metadata": {
        "id": "044957ff-8ed6-4fc7-9827-55ebf1f8b3a2"
      },
      "source": [
        "Compute the negative log-likelihood of the validation data under each such $n$-gram model with $n \\in \\{ 1, 2, 3, 4, 5 \\}$. Add a method `model.block_size() == n-1` returning the maximal size of the context that your model can handle, to avoid a quadratic complexity in the likelihood computation. How do $5$-gram models compare to the previous bigram model on the \"Juliet\" test ? Does the generated text look better ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e98c0f-f143-42be-ae18-486846235ffe",
      "metadata": {
        "id": "68e98c0f-f143-42be-ae18-486846235ffe"
      },
      "outputs": [],
      "source": [
        "### your code here ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a07cbdb-c851-4bf9-83f1-3512cff71f2e",
      "metadata": {
        "id": "8a07cbdb-c851-4bf9-83f1-3512cff71f2e"
      },
      "source": [
        "## Part C - Language Modeling with Transformers\n",
        "\n",
        "### C.1 - Training setup with bigram models\n",
        "\n",
        "To prepare the setup for training language models by deep learning techniques, but before diving into the details of the model, let us re-build the bigram model in a learning-friendly manner,\n",
        "and learn the transition probability matrix to maximize the log-likelihood of the training data, instead of computing the empirical probabilities manually.\n",
        "\n",
        "We provide the `get_batch` and `train_model` functions.\n",
        "We will compute predictions for all elements of the batch and for all timesteps at once, to speed up training.\n",
        "This means for a batch size of 1 and an alphabet of size 65, the prediction for `xb.shape = (1, block_size)`\n",
        "should have `model(xb).shape = (1, block_size, 65)`.\n",
        "Read carefully the shape assertion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39637c08-2f54-4a99-876e-b62b5743ed36",
      "metadata": {
        "id": "39637c08-2f54-4a99-876e-b62b5743ed36"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, batch_size = 16, block_size = 32):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f06c090-c7df-4aa7-8ce0-b596f5984f0d",
      "metadata": {
        "id": "8f06c090-c7df-4aa7-8ce0-b596f5984f0d"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, iterations=1):\n",
        "    for _ in range(iterations):\n",
        "        xb, yb = get_batch(train_data)\n",
        "        logits = model(xb)\n",
        "        assert logits.shape == (*yb.shape, len(alphabet.characters))\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "        loss = F.cross_entropy(logits, yb.view(-1))\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67617f4-1e88-41ac-8e3e-f7ae01e996ff",
      "metadata": {
        "id": "d67617f4-1e88-41ac-8e3e-f7ae01e996ff"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_negloglikelihood(model, data, eval_iters=250):\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data)\n",
        "        logits = model(X)\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "        loss = F.cross_entropy(logits, Y.view(-1))\n",
        "        losses[k] = loss.item()\n",
        "    model.train()\n",
        "    return losses.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f6dbb0-dc86-4e07-9099-a997cc0c882d",
      "metadata": {
        "id": "71f6dbb0-dc86-4e07-9099-a997cc0c882d"
      },
      "source": [
        "Write the Bigram model as a trainable `torch.nn.Module`. Make sure it works with batches of shape `(batch_size, block_size)`, for which the forward function should return a tensor of shape `(batch_size, block_size, alphabet_size)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721590dd-182d-45b0-8380-751917bf3e3d",
      "metadata": {
        "id": "721590dd-182d-45b0-8380-751917bf3e3d"
      },
      "outputs": [],
      "source": [
        "class Bigram(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "    ### you code here ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632e7d2f-7b74-420b-bcb4-1c7eca014deb",
      "metadata": {
        "id": "632e7d2f-7b74-420b-bcb4-1c7eca014deb"
      },
      "outputs": [],
      "source": [
        "bigram_model = Bigram(len(alphabet.characters)).to(device)\n",
        "print(f\"Bigram model has {TP07.printable_parameter_count(bigram_model)} parameters\")\n",
        "optimizer = torch.optim.Adam(bigram_model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0774a0d-b690-467f-a2cd-a8cf8f1c9ec0",
      "metadata": {
        "id": "b0774a0d-b690-467f-a2cd-a8cf8f1c9ec0"
      },
      "outputs": [],
      "source": [
        "for iter in range(5):\n",
        "    bigram_model, optimizer = train_model(bigram_model, optimizer, iterations=100)\n",
        "    train_loss, val_loss = [ estimate_negloglikelihood(bigram_model, data) for data in [ train_data, val_data ] ]\n",
        "    print(f\"train loss {train_loss:.4f}, val loss {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc8668e-eb2c-4f6a-9016-88629b300c33",
      "metadata": {
        "id": "6bc8668e-eb2c-4f6a-9016-88629b300c33"
      },
      "outputs": [],
      "source": [
        "wrapped_bigram_model = lambda u: bigram_model(u)[-1]\n",
        "print( prompt(wrapped_bigram_model, 100, preprompt=\"Jul\", temp=1e-2) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c581ae-20eb-4446-a6c4-463c61b571b0",
      "metadata": {
        "id": "74c581ae-20eb-4446-a6c4-463c61b571b0"
      },
      "source": [
        "### C.2 - Advanced Language Models\n",
        "\n",
        "Now let us build a GPT-like architecture.\n",
        "We will use the causal / masked attention operation,\n",
        "where the mask $M \\in \\{ 0 ,  1\\}^{d \\times d}$ is defined as $M_{i,j} = 1$ if and only if $i > j$.\n",
        "$$ \\operatorname{Attention} : X \\in \\mathbb{R}^{d \\times d} \\mapsto \\left[ \\dfrac{M_{i,j} \\exp(X_{i,j}) }{ \\sum_{k} M_{i,k} \\exp(X_{i,k}) } \\right]_{i,j} \\in \\mathbb{R}^{d \\times d} $$\n",
        "If you want use `F.softmax`, you may want to use the functions `Tensor.masked_fill` and `torch.tril` as well.\n",
        "You can implement matrix multiplication with a learnable weight with `torch.nn.Linear(bias=false)`.\n",
        "\n",
        "We write $B$ for the batch size, $T$ for the block size (or \"timesteps\"), and $C$ for the embedding dimension (or number of \"channels\").\n",
        "\n",
        "1. Write a SelfAttention module implementing the operation with weights $K \\in \\mathbb{R}^{C \\times d}$, $Q \\in \\mathbb{R}^{C \\times d}$ and $V \\in \\mathbb{R}^{C \\times d}$\n",
        "$$ X \\in \\mathbb{R}^{B \\times T \\times C} \\mapsto \\left[ \\operatorname{Attention}\\left[\\frac{(X_i \\cdot K) \\cdot (X_i \\cdot Q)^T}{\\sqrt{d}}\\right] \\cdot (X_i \\cdot V) \\right]_{i \\in [B]} $$\n",
        "We choose the variable `d` such that `d * num_heads = n_embd`, so the size is unchanged after going through a multi-head block.\n",
        "\n",
        "3. Write a Multi-Head attention module computing the output of `num_heads` independent Self-Attention modules, then concatenating the result.\n",
        "\n",
        "4. Write a Transformer module, computing the composition of a residual block with a multi-head attention module as residual,\n",
        "    followed by a residual block with a two-layer ReLU-network for the residual. Add a layer norm at the start of each residual branch to avoid training instabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785bf110-29d5-4936-98e8-c514671f1b06",
      "metadata": {
        "id": "785bf110-29d5-4936-98e8-c514671f1b06"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "\n",
        "    ### your code here ###\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "\n",
        "    ### your code here ###\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "    ### your code here ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f809ea-955f-4a64-8ab3-25b526b0a0ff",
      "metadata": {
        "id": "42f809ea-955f-4a64-8ab3-25b526b0a0ff"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, hidden_size, n_layers, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        blocks = [\n",
        "            TransformerBlock(n_embd=n_embd,\n",
        "                             n_head=n_head,\n",
        "                             block_size=block_size,\n",
        "                             hidden_size=hidden_size)\n",
        "            for _ in range(n_layers) ])\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self._block_size = block_size\n",
        "\n",
        "    def block_size(self):\n",
        "        return self._block_size\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        # idx is a (B,T)-shaped LongTensor\n",
        "        tok_emb = self.token_embedding_table(idx) # shape: (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # shape: (T,C)\n",
        "        x = tok_emb + pos_emb # shape: (B,T,C)\n",
        "        x = self.blocks(x) # shape: (B,T,C)\n",
        "        x = self.ln_f(x) # shape: (B,T,C)\n",
        "        logits = self.lm_head(x) # shape: (B,T,vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3742077e-19f4-4aed-b4ed-2a6bec9c8c99",
      "metadata": {
        "id": "3742077e-19f4-4aed-b4ed-2a6bec9c8c99"
      },
      "source": [
        "---\n",
        "\n",
        "Initialize your language model, and train it repeatedly or tweak the hyperparameters and retrain, until you reach training losses below 1.5 nats/char. Prompt regularly during training to witness the evolution of the generated samples, but don't waste your time budget sampling too much if your training loss is above 1.75, you would get mostly unintelligible gibberish.\n",
        "\n",
        "You should be able, by the end of the session, to get didaskalia in capitals with real Shakespeare character names followed by a colon, and vaguely old-english-sounding sentences separated by dots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc20468c-e1e0-4adc-a191-c1944ea8f37a",
      "metadata": {
        "id": "fc20468c-e1e0-4adc-a191-c1944ea8f37a"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"n_head\": 4,\n",
        "    \"n_embd\": 64,\n",
        "    \"hidden_size\": 1024,\n",
        "    \"n_layers\": 2,\n",
        "    \"block_size\": 32,\n",
        "    \"vocab_size\": len(alphabet.characters),\n",
        "}\n",
        "language_model = LanguageModel(**config).to(device)\n",
        "paramcount = TP07.printable_parameter_count(language_model)\n",
        "print(f\"Language model has {paramcount} parameters\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(language_model.parameters(), lr=1e-3)\n",
        "total_iter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6bebe5-684a-4853-9b45-94ee323d3b48",
      "metadata": {
        "id": "7d6bebe5-684a-4853-9b45-94ee323d3b48"
      },
      "outputs": [],
      "source": [
        "max_iters, eval_interval = 1000, 100\n",
        "\n",
        "for iter in range(max_iters // eval_interval):\n",
        "    language_model, optimizer = train_model(language_model, optimizer, iterations=eval_interval)\n",
        "    train_loss, val_loss = [ estimate_negloglikelihood(language_model, data) for data in [ train_data, val_data ] ]\n",
        "    total_iter += eval_interval\n",
        "    print(f\"step {total_iter:4d}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581362f2-f312-4af8-8812-d66f2dd2869b",
      "metadata": {
        "id": "581362f2-f312-4af8-8812-d66f2dd2869b"
      },
      "outputs": [],
      "source": [
        "def lm_wrapper(m, u):\n",
        "    u = torch.LongTensor(u)[None, -m.block_size():]\n",
        "    return m(u)[0,-1,:]\n",
        "\n",
        "print( prompt(lambda u: lm_wrapper(language_model, u), 100, preprompt=\"Jul\") )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19bc215e-24fc-40f7-b4c9-40b2f2bd8e31",
      "metadata": {
        "id": "19bc215e-24fc-40f7-b4c9-40b2f2bd8e31"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}