{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJH7qk7QLLDVRfTW3YdCuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kscaman/MAP583/blob/main/TP/TP06_stability_robustness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP06 - Stability and robustness\n",
        "In this practical, we are going to investigate the stability and robustness of neural networks, during training and at inference. To do so, we will test different initialization schemes, see their effect on the regularity of the function, test different methods to improve stability in the presence of outliers in the training distribution, and learn to generate adversarial attacks on pre-trained models."
      ],
      "metadata": {
        "id": "A3MoeiO7Jfww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnktggQTIBiT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A - Weight initialization\n",
        "Let's now investigate the effect of initialization on simple neural networks (MLPs).\n",
        "\n",
        "## A.0 - Model creation\n",
        "First, we need to automatically create large and deep MLPs. Create a function `MLP(dim_input, dim_output, dim_hidden, num_layers)` that returns an MLP with ReLU activations, `num_layers` layers and width `dim_hidden` using `nn.Sequential`.\n",
        "\n",
        "Check that the MLP has the correct architecture for 1, 2 and 4 layers."
      ],
      "metadata": {
        "id": "CrnrhOhxICBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8rA7GbucmSnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.1 - Neural networks at initialization\n",
        "We are now going to experiment with initialization. First, let's plot the function created by an MLP at initialization."
      ],
      "metadata": {
        "id": "k6aWQ4Kx3EZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "qlilulhZ484F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot multiple functions on the same figure."
      ],
      "metadata": {
        "id": "lCU-tKVV7snm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "8m1q5N2y4MjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of layers to 10. What happens? Is that a problem for learning?"
      ],
      "metadata": {
        "id": "kBKFLf5v72UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L4ecdddi8Acc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to fix this issue by applying a different initialization.\n",
        "Create a function that initializes all weights of the MLP by using functions in [`nn.init`](https://pytorch.org/docs/stable/nn.init.html)."
      ],
      "metadata": {
        "id": "hjqr-Pgd8Llb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "model = MLP(1, 1 , 100, 10)\n",
        "for _ in range(10):\n",
        "    model.apply(init_weights)\n",
        "    x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "    y = model(x)\n",
        "\n",
        "    plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D4q1_2JFnVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.2 - Fixing the issue with batch normalization\n",
        "Add a batch norm `nn.BatchNorm1d` layer after each hidden layer."
      ],
      "metadata": {
        "id": "oUE4qBqB5xj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "AkW4mGPm5wDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the result different at initialization? Plot several functions generated by a 10-layer MLP at initialization (with default initialization)."
      ],
      "metadata": {
        "id": "Qksx_9L864FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "L0w9gDmo6hzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš  **Careful though:** Batch norm depends on the **whole batch**, and uses the **training mean and standard deviation** during **evaluation**."
      ],
      "metadata": {
        "id": "4RU8WzYYDDrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1,1]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_e_FjLD8ThgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH TRAINING DATASET ON [-1e-3,1e-3]\n",
        "model = MLP_bn(1, 1 , 100, 10)\n",
        "model.train()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "for _ in range(1000):\n",
        "    y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "x = torch.linspace(-1e-3, 1e-3, 100).view(-1, 1)\n",
        "y = model(x)\n",
        "\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2-E6VybXmp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B - Stability, outliers and overfitting\n",
        "We now investigate the generaliation capabilities of MLPs on a simple regression task: our aim is to lear the function $y=\\sin(3 x)$. However, a small number of training samples (denoted as **outliers**) were randomly perturbed by a large factor (of order 100)."
      ],
      "metadata": {
        "id": "9GUj2cJzhjM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "num_points = 200\n",
        "outlier_std = 100\n",
        "num_outliers = 5\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "x_train = 4 * (2 * torch.rand(num_points, 1) - 1)\n",
        "outlier_noise = torch.zeros_like(x_train)\n",
        "outlier_noise[torch.randperm(num_points)[:num_outliers]] = outlier_std * torch.randn(num_outliers, 1)\n",
        "y_train_clean = torch.sin(3 * x_train)\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train_clean, outlier_noise)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "x_test = 4 * (2 * torch.rand(100, 1) - 1)\n",
        "y_test = torch.sin(3 * x_test)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "def plot_function(model=None, zoom=True):\n",
        "    x = torch.linspace(-4, 4, 1000)\n",
        "    plt.plot(x_train, y_train_clean + outlier_noise, '.', label=\"train\")\n",
        "    plt.plot(x_test, y_test, '.', label=\"test\")\n",
        "    plt.plot(x, torch.sin(3 * x), label=\"target\")\n",
        "    if model is not None:\n",
        "        output = model(x.unsqueeze(1).to(device)).cpu().detach().numpy()\n",
        "        plt.plot(x, output, label=\"model\")\n",
        "    plt.legend()\n",
        "    if zoom:\n",
        "        plt.ylim([-2, 2])\n",
        "    plt.xlabel('input value ($x$)')\n",
        "    plt.ylabel('function value ($f(x)$)')\n",
        "    plt.show()\n",
        "\n",
        "plot_function(zoom=False)"
      ],
      "metadata": {
        "id": "LFWCOcwxuO3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create three functions:\n",
        "\n",
        "1.   A function `train_epoch(model, optimizer, clipping_threshold=None)` that trains the model for one epoch and applies gradient clipping if `clipping_threshold` is not `None`.\n",
        "2.   A function `test(model)` that returns the loss over the test set.\n",
        "3.   A function `train(model, optimizer, num_epochs, clipping_threshold=None)` that performs `num_epochs` epochs of training (with the MSE loss), and plots (in a figure) the train and test losses at each epoch.\n"
      ],
      "metadata": {
        "id": "cYknu6SQo5O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, clipping_threshold):\n",
        "    model.train()\n",
        "    loss_function = nn.MSELoss(reduction='sum')\n",
        "    losses = []\n",
        "    for input, target, outlier_noise in train_dataloader:\n",
        "        input, target, outlier_noise = input.to(device), target.to(device), outlier_noise.to(device)\n",
        "        output = model(input)\n",
        "        target_with_outliers = target + outlier_noise\n",
        "        ### YOUR CODE HERE ###\n",
        "    return np.sum(losses) / len(train_dataloader)\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    loss_function = nn.MSELoss(reduction='sum')\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for input, target in test_dataloader:\n",
        "            ### YOUR CODE HERE ###\n",
        "    return np.sum(losses) / len(test_dataloader)\n",
        "\n",
        "def train(model, optimizer, num_epochs, clipping_threshold=None):\n",
        "    ### YOUR CODE HERE ###\n",
        "    pass"
      ],
      "metadata": {
        "id": "NRQlD8QThvKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train an MLP of width $10^3$ and depth $4$ with Adam (lr=1e-3), and plot the target function and the output of the model. The model tries to fit the outliers, lead to a poor performance on the test set."
      ],
      "metadata": {
        "id": "arYGo3-zhzwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "kDhfsM8Midqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to test 3 methods: 1) adding batch norm layers, 2) adding gradient clipping, and 3) adding regularization (aka weight decay). Test all three methods. Try to find reasonable parameters for clipping and weight decay by hand. Is the output of the model smoother? Can we reduce the impact of outliers?"
      ],
      "metadata": {
        "id": "1xSlTMznh-7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "KgGSfKOV23gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with all three methods toghether. Is the test error better?"
      ],
      "metadata": {
        "id": "_PM5JJGv4cqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "pdtcmrp94lPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus question\n",
        "Use a validation set to find optimal parameters for the learning rate, weigth decay and gradient clipping."
      ],
      "metadata": {
        "id": "5-cJgLjI3Gmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "USiXmWTr3YbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy75YWS9vsb9"
      },
      "source": [
        "# Part C - Adversarial examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D_OaiLBvsb9"
      },
      "source": [
        "In this part, we will look at [adversarial examples](https://arxiv.org/abs/1607.02533): \"An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems...\"\n",
        "\n",
        "Rules of the game:\n",
        "- the attacker cannot modify the classifier, i.e. the neural net with the preprocessing done on the image before being fed to the network.\n",
        "- even if the attacker cannot modify the classifier, we assume that the attacker knows the architecture of the classifier. Here, we will work with `resnet18` and the standard Imagenet normalization.\n",
        "- the attacker can only modify the physical image fed into the network.\n",
        "- the attacker should fool the classifier, i.e. the label obtained on the corrupted image should not be the same as the label predicted on the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jPbZAP9vsb-"
      },
      "outputs": [],
      "source": [
        "# Image under attack!\n",
        "url_car = 'https://cdn130.picsart.com/263132982003202.jpg?type=webp&to=min&r=640'\n",
        "response = requests.get(url_car)\n",
        "img_pil = Image.open(io.BytesIO(response.content))\n",
        "plt.imshow(img_pil);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W03j6WWAvsb-"
      },
      "outputs": [],
      "source": [
        "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "preprocess = torchvision.transforms.Compose([\n",
        "   torchvision.transforms.Resize((224,224)),\n",
        "   torchvision.transforms.ToTensor(),\n",
        "   normalize\n",
        "])\n",
        "\n",
        "net = torchvision.models.resnet18(pretrained=True)\n",
        "net.eval()\n",
        "\n",
        "for p in net.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "x = preprocess(img_pil).clone().unsqueeze(0)\n",
        "scores = net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI8zw098vsb-"
      },
      "outputs": [],
      "source": [
        "# download the imagenet category list\n",
        "LABELS_URL = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
        "classes = {int(key):value for (key, value)\n",
        "          in requests.get(LABELS_URL).json().items()}\n",
        "\n",
        "def print_preds(scores):\n",
        "    # print the predictions with their 'probabilities' from the scores\n",
        "    h_x = F.softmax(scores, dim=1).data.squeeze()\n",
        "    probs, idx = h_x.sort(0, True)\n",
        "    probs = probs.numpy()\n",
        "    idx = idx.numpy()\n",
        "    # output the prediction\n",
        "    for i in range(0, 5):\n",
        "        print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
        "    return idx\n",
        "\n",
        "_ = print_preds(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsVGqcW8vsb-"
      },
      "outputs": [],
      "source": [
        "t_std = torch.Tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "t_mean = torch.Tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "\n",
        "def plot_img_tensor(img):\n",
        "    plt.imshow(np.transpose(img.detach().numpy(), [1,2,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfoAW7XYvsb-"
      },
      "outputs": [],
      "source": [
        "# here we display an image given as a tensor\n",
        "x_img = (x * t_std + t_mean).squeeze(0)\n",
        "plot_img_tensor(x_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nINCxpkvsb9"
      },
      "source": [
        "## C.1 - Creating the attack\n",
        "First, you will implement *Fast gradient sign method (FGSM)* which is described in Section 2.1 of [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533). The idea is simple, suppose you have an image $\\mathbf{x}$ and when you pass it through the network, you get the 'true' label $y$. You know that your network has been trained by minimizing the loss $J(\\mathbf{\\theta}, \\mathbf{x}, y)$ with respect to the parameters of the network $\\theta$. Now, $\\theta$ is fixed as you cannot modify the classifier so you need to modify $\\mathbf{x}$. In order to do so, you can compute the gradient of the loss with respect to $\\mathbf{x}$ i.e. $\\nabla_{\\mathbf{x}} J(\\mathbf{\\theta}, \\mathbf{x}, y)$ and use it as follows to get the modified image $\\tilde{\\mathbf{x}}$:\n",
        "$$\n",
        "\\tilde{\\mathbf{x}} = \\text{Clamp}\\left(\\mathbf{x} + \\epsilon *\n",
        "\\text{sign}(\\nabla_{\\mathbf{x}} J(\\mathbf{\\theta}, \\mathbf{x}, y)),0,1\\right),\n",
        "$$\n",
        "where $\\text{Clamp}(\\cdot, 0,1)$ ensures that $\\tilde{\\mathbf{x}}$ is a proper image.\n",
        "Note that if instead of sign, you take the full gradient, you are now following the gradient i.e. increasing the loss $J(\\mathbf{\\theta}, \\mathbf{x}, y)$ so that $y$ becomes less likely to be the predicted label.\n",
        "\n",
        "1. Implement this attack. Make sure to display the corrupted image.\n",
        "\n",
        "2. For what value of epsilon is your attack successful? What is the predicted class then?\n",
        "\n",
        "3. Plot the sign of the gradient and pass this image through the network. What prediction do you obtain? Compare to [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2hEfcQhvsb-"
      },
      "outputs": [],
      "source": [
        "# your implementation of the attack\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9cmdqCyvsb-"
      },
      "outputs": [],
      "source": [
        "idx = 656 #minivan\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "x_img.requires_grad = True\n",
        "scores = net(normalize(x_img).unsqueeze(0))\n",
        "target = torch.tensor([idx])\n",
        "\n",
        "### YOUR CODE HERE: compute the loss to backpropagate ###\n",
        "\n",
        "_ = print_preds(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itl8WY_Cvsb-"
      },
      "outputs": [],
      "source": [
        "# your attack here\n",
        "epsilon = 0\n",
        "x_att = fgsm_attack(x_img,epsilon,?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqqOfG-Yvsb_"
      },
      "outputs": [],
      "source": [
        "# the new prediction for the corrupted image\n",
        "scores = net(normalize(x_att).unsqueeze(0))\n",
        "_ = print_preds(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBoSVUwzvsb_"
      },
      "outputs": [],
      "source": [
        "# can you see the difference?\n",
        "plot_img_tensor(x_att)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8wkvdrmvsb_"
      },
      "outputs": [],
      "source": [
        "# do not forget to plot the sign of the gradient\n",
        "gradient = ### YOUR CODE HERE ###\n",
        "plot_img_tensor((1+gradient)/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wovP1upFvsb_"
      },
      "outputs": [],
      "source": [
        "# what is the prediction for the gradient?\n",
        "scores = net(normalize(gradient).unsqueeze(0))\n",
        "_ = print_preds(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4yis0mvscA"
      },
      "source": [
        "## C.2 - Transforming a car into a cat\n",
        "\n",
        "We now implement the *Iterative Target Class Method (ITCM)* as defined by equation (4) in [Adversarial Attacks and Defences Competition](https://arxiv.org/abs/1804.00097)\n",
        "\n",
        "To test it, we will transform the car (labeled minivan by our `resnet18`) into a [Tabby cat](https://en.wikipedia.org/wiki/Tabby_cat) (classe 281 in Imagenet). But you can try with any other target.\n",
        "\n",
        "Implement the ITCM and make sure to display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDoFXXzHvscA"
      },
      "outputs": [],
      "source": [
        "x = preprocess(img_pil).clone()\n",
        "xd = preprocess(img_pil).clone()\n",
        "xd.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMTXoewavscA"
      },
      "outputs": [],
      "source": [
        "idx = 281 #tabby\n",
        "optimizer = optim.SGD([xd], lr=0.01)\n",
        "\n",
        "for i in range(200):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss.item())\n",
        "\n",
        "    _ = print_preds(output)\n",
        "    print(i,'-----------------')\n",
        "\n",
        "    # TODO: break the loop once we are satisfied\n",
        "    if ?:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbkwT28CvscA"
      },
      "outputs": [],
      "source": [
        "_ = print_preds(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dadtriqXvscA"
      },
      "outputs": [],
      "source": [
        "# plot the corrupted image\n"
      ]
    }
  ]
}